{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Electronic Technologies and Biosensors Laboratory\n",
    "##### Project 5 – Sleep Position Classification\n",
    "##### Alfonzo Massimo, Canavate Chloé, Franke Patrick\n",
    "##### Juli 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1a4KEXnKmApS"
   },
   "source": [
    "## Library import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x3FXmBv4zcdF"
   },
   "outputs": [],
   "source": [
    "# Data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from os import path\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Models\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, classification_report\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, precision_recall_curve, roc_curve, roc_auc_score\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from scipy.cluster.hierarchy import ward\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gtiRkxxGmI8W"
   },
   "source": [
    "## Data import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "JWSfxq6Czknn",
    "outputId": "0d291bca-06d6-4b7c-9f10-c2394222af29"
   },
   "outputs": [],
   "source": [
    "# Path to data\n",
    "path_folder = 'dataset/'\n",
    "\n",
    "# Paths to subjects data\n",
    "# Female\n",
    "path_subject_f01 = path.join(path_folder, 'alessandra_20220601_182359.csv')\n",
    "path_subject_f02 = path.join(path_folder, 'aurora_20220601_180419.csv')\n",
    "path_subject_f03 = path.join(path_folder, 'benedetta_20220601_184141.csv')\n",
    "path_subject_f04 = path.join(path_folder, 'chloe_20220601_165046.csv')\n",
    "path_subject_f05 = path.join(path_folder, 'giulia_20220601_171812.csv')\n",
    "path_subject_f06 = path.join(path_folder, 'laura_20220601_174539.csv')\n",
    "path_subject_f07 = path.join(path_folder, 'melissa_20220601_190124.csv')\n",
    "\n",
    "# Male\n",
    "path_subject_m01 = path.join(path_folder, 'axel_20220610_193124.csv')\n",
    "path_subject_m02 = path.join(path_folder, 'fabio_20220601_193018.csv')\n",
    "path_subject_m03 = path.join(path_folder, 'lilian_20220610_214447.csv')\n",
    "path_subject_m04 = path.join(path_folder, 'massimo_20220601_194803.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "3tyEsY5T0oGa",
    "outputId": "64421b18-778b-4502-dbf3-449918e64e40"
   },
   "outputs": [],
   "source": [
    "# Data reading\n",
    "# Female\n",
    "data_subject_f01 = pd.read_csv(path_subject_f01)\n",
    "data_subject_f02 = pd.read_csv(path_subject_f02)\n",
    "data_subject_f03 = pd.read_csv(path_subject_f03)\n",
    "data_subject_f04 = pd.read_csv(path_subject_f04)\n",
    "data_subject_f05 = pd.read_csv(path_subject_f05)\n",
    "data_subject_f06 = pd.read_csv(path_subject_f06)\n",
    "data_subject_f07 = pd.read_csv(path_subject_f07, on_bad_lines='skip') # --> Melissa's file is corrupted, missing positions from 2 to 4\n",
    "\n",
    "# Male\n",
    "data_subject_m01 = pd.read_csv(path_subject_m01, sep=';')\n",
    "data_subject_m02 = pd.read_csv(path_subject_m02)  # --> Fabio's file is corrupted, sensor detached for positions 4 and 9\n",
    "data_subject_m03 = pd.read_csv(path_subject_m03, sep=';')\n",
    "data_subject_m04 = pd.read_csv(path_subject_m04)\n",
    "\n",
    "data_subjects = [data_subject_f01, data_subject_f02, data_subject_f03, data_subject_f04, data_subject_f05, data_subject_f06, data_subject_f07, data_subject_m01, data_subject_m02, data_subject_m03, data_subject_m04]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "47_bFUv1mMBN"
   },
   "source": [
    "## Data visualization and cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section aims to visualize the data collected, to look for potential outliers, and to prepare the final dataset that will be fed to the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 862
    },
    "id": "aCUUZnsomQy8",
    "outputId": "1ba3f849-6a60-4f23-9a15-151af4baa83a"
   },
   "outputs": [],
   "source": [
    "# Plot acceleration data for the 12 consecutive positions for each subject\n",
    "\n",
    "fig, axs = plt.subplots(3, 4, figsize=(25,13))\n",
    "axs = axs.flatten()\n",
    "i = 0\n",
    "subject_nb = 1\n",
    "\n",
    "for data_subject in data_subjects:\n",
    "  axs[i].plot(data_subject[['x_chest', 'y_chest', 'z_chest', 'x_ankle', 'y_ankle', 'z_ankle']])\n",
    "  axs[i].set_title('Subject {}'.format(subject_nb))\n",
    "  axs[i].set_ylabel('Acceleration')\n",
    "  subject_nb += 1\n",
    "  i += 1\n",
    "\n",
    "fig.legend(['x_chest', 'y_chest', 'z_chest', 'x_ankle', 'y_ankle', 'z_ankle'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these graphs, it can be seen that for each subject, changing position, even though it was not recorded in the csv file, has resulted in some transient phases for the accelerometers to stabilize. \\\n",
    "Let's remove these phases by considering they span on at most 20 samples at the beginning of each position recording."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "eW1cMWSoHwaz",
    "outputId": "5efae290-51a3-49d0-e5a1-639932531b32"
   },
   "outputs": [],
   "source": [
    "# Remove the transient parts in the data and create new datasets for each patient\n",
    "\n",
    "subject_list = ['f01', 'f02', 'f03', 'f04', 'f05', 'f06', 'f07', 'm01', 'm02', 'm03', 'm04']\n",
    "data_subjects_clean = []\n",
    "position_indexes = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "\n",
    "for i in range(len(subject_list)):\n",
    "\n",
    "  # Clean the subject's data by removing the transient portions\n",
    "  subject_data = pd.DataFrame(data_subjects[i])\n",
    "  subject_data_clean = pd.DataFrame()\n",
    "\n",
    "  for pos_idx in position_indexes:\n",
    "    position_data = subject_data.loc[subject_data['position']==pos_idx]\n",
    "\n",
    "    if position_data.shape[0] != 0:\n",
    "\n",
    "      # Remove the 20 first rows for each position if more than 20 samples are present\n",
    "      if position_data.shape[0] > 20:\n",
    "        position_data_cleaned = position_data.iloc[20:,:]\n",
    "        \n",
    "      # If there is no 20 records for that subject and position, keep the raw data \n",
    "      else:\n",
    "        position_data_cleaned = position_data\n",
    "\n",
    "    subject_data_clean = pd.concat([subject_data_clean, position_data_cleaned], axis=0)   \n",
    "\n",
    "  data_subjects_clean.append(pd.DataFrame(subject_data_clean))\n",
    "    \n",
    "data_subjects_clean[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 862
    },
    "id": "r-OkSl07JMiT",
    "outputId": "3240bf0d-ab1d-4bbc-e110-50371743e4d6"
   },
   "outputs": [],
   "source": [
    "# Plot acceleration data for the 12 consecutive positions for each subject, after removal of the transient phases\n",
    "\n",
    "fig, axs = plt.subplots(3, 4, figsize=(25,13))\n",
    "axs = axs.flatten()\n",
    "i = 0\n",
    "subject_nb = 1\n",
    "\n",
    "for data_subject in data_subjects_clean:\n",
    "  axs[i].plot(data_subject[['x_chest', 'y_chest', 'z_chest', 'x_ankle', 'y_ankle', 'z_ankle']])\n",
    "  axs[i].set_title('Subject {}'.format(subject_nb))\n",
    "  axs[i].set_ylabel('Acceleration')\n",
    "  subject_nb += 1\n",
    "  i += 1\n",
    "\n",
    "fig.legend(['x_chest', 'y_chest', 'z_chest', 'x_ankle', 'y_ankle', 'z_ankle'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these new graphs, it can be noticed that the lines were smoothed. \\\n",
    "These plots also show that the subject 9 presents erroneous data, as expected, since during data acquisition sensors fell off several times. \\\n",
    "Let's combine all the datasets together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "DIl3-W_TOApC",
    "outputId": "10e40dfa-7d92-4d46-8a78-bda91fd25359"
   },
   "outputs": [],
   "source": [
    "all_data = pd.DataFrame()\n",
    "id = 1\n",
    "\n",
    "for data_subject in data_subjects_clean:\n",
    "  data_subject['subject_id'] = [id for i in range(data_subject.shape[0])]   # Add subject_id column to the concatenated dataset\n",
    "  id += 1\n",
    "  all_data = pd.concat([all_data, data_subject])\n",
    "\n",
    "all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important step of the data preparation is to standardize it. Even though in our case all the features that will be used by the model are on the same scale (6 acceleration coordinates), it is always better to scale them in order to have lower values, helping the models to learn. \\\n",
    "This standardization aims to visualize only the data thanks to boxplots. Later on, the standardization will be performed on the created datasets, paying attention to the train-test splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "luPbxLsyQmbh",
    "outputId": "b6506e1f-64e1-4e6f-db81-1d53557def9c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(all_data.iloc[:,:6])\n",
    "all_data.iloc[:,:6] = scaler.transform(all_data.iloc[:,:6])\n",
    "\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 748
    },
    "id": "Xwljmt1eMRDq",
    "outputId": "c61fb42d-92ca-4ce3-89a3-2045c75bf6c5"
   },
   "outputs": [],
   "source": [
    "# Boxplots of the acceleration data grouped by subject, for each position and each x/y/z\n",
    "\n",
    "all_data_copy = all_data.copy()\n",
    "position_to_plot = 11   # Change here the position to plot\n",
    "\n",
    "# Select the position requested\n",
    "all_data_copy = all_data_copy.loc[all_data_copy.position == position_to_plot]\n",
    "all_data_copy.drop(['position'], axis=1, inplace=True)\n",
    "\n",
    "all_data_melt = pd.melt(all_data_copy, id_vars=['subject_id'])\n",
    "all_data_melt.columns = ['subject_id', 'variable', 'acceleration']\n",
    "plt.figure(figsize=(20,12))\n",
    "sns.boxplot(x='variable', y='acceleration', hue='subject_id', data=all_data_melt)\n",
    "plt.title('Position {}'.format(position_to_plot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WPG3uaeCKU7q"
   },
   "source": [
    "From the boxplots, by varying the positions from 1 to 12, we noticed the following anomalies:\n",
    "\n",
    "\n",
    "*   Positions 4 and 9, Subject 9\n",
    "*   Position 8, Subject 11\n",
    "*   Position 10, y_ankle\n",
    "*   Position 11, Subject 7\n",
    "*   Position 12, Subject 5\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qCoBK2tzpXLD"
   },
   "source": [
    "## Features computation and dataset creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4J1ANBxgqmsR"
   },
   "source": [
    "After having performed data visualization, this section aims to start back with the raw data and apply the needed transformations to create our datasets. The transient phases between positions are removed and the averages, min and max of acceleration data are extracted as features. \\\n",
    "Three datasets are created:\n",
    "\n",
    "\n",
    "*   A dataset containing all the samples for the 6 coordinates except the transient phases - Dataset 1\n",
    "*   A dataset containing the averages, the min and the max of the 6 coordinates - Dataset 2\n",
    "*   A dataset containing only the averages of the 6 coordinates - Dataset 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "ss9S4QsTskpY",
    "outputId": "620a9bba-eaa2-4241-a4d3-2770b95ad27c"
   },
   "outputs": [],
   "source": [
    "# Remove the transient samplings between 2 positions for each subject and create a dataset\n",
    "\n",
    "subject_list = ['f01', 'f02', 'f03', 'f04', 'f05', 'f06', 'f07', 'm01', 'm02', 'm03', 'm04']\n",
    "id = 1\n",
    "position_indexes = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "dataset = pd.DataFrame()\n",
    "\n",
    "for i in range(len(subject_list)):\n",
    "  # Get identifier and sex of the subject\n",
    "  sex = subject_list[i][0].upper()\n",
    "\n",
    "  # Clean the subject's data by removing the transient portions\n",
    "  subject_data = pd.DataFrame(data_subjects[i].copy())\n",
    "  subject_data['subject_id'] = [id for i in range(subject_data.shape[0])]\n",
    "  subject_data['sex'] = [sex for i in range(subject_data.shape[0])]\n",
    "\n",
    "  for pos_idx in position_indexes:\n",
    "    position_data = subject_data.loc[subject_data['position']==pos_idx]\n",
    "\n",
    "    if position_data.shape[0] != 0:\n",
    "\n",
    "      # Remove the 20 first rows for each position if more than 20 samples are present\n",
    "      if position_data.shape[0] > 20:\n",
    "        position_data_cleaned = position_data.iloc[20:,:]\n",
    "        \n",
    "      else:\n",
    "        position_data_cleaned = position_data\n",
    "\n",
    "      dataset = pd.concat([dataset, position_data_cleaned])\n",
    "\n",
    "  id += 1\n",
    "  \n",
    "dataset.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6SkFUj5jx3_k",
    "outputId": "da67d717-eeed-49ae-b7ce-be722f05f8ce"
   },
   "outputs": [],
   "source": [
    "dataset.subject_id.value_counts()   # Sanity check of correct dataset creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "IwZokAxG1s6x",
    "outputId": "774ada61-a49f-4da6-fcea-1ee8ea3accb4"
   },
   "outputs": [],
   "source": [
    "# Remove the transient samplings between 2 positions for each subject, take the averages and concatenate data into a single dataset\n",
    "\n",
    "subject_list = ['f01', 'f02', 'f03', 'f04', 'f05', 'f06', 'f07', 'm01', 'm02', 'm03', 'm04']\n",
    "id = 1\n",
    "acceleration_avgs = []\n",
    "position_indexes = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "\n",
    "for i in range(len(subject_list)):\n",
    "  # Get identifier and sex of the subject\n",
    "  sex = subject_list[i][0].upper()\n",
    "\n",
    "  # Clean the subject's data by removing the transient portions\n",
    "  subject_data = pd.DataFrame(data_subjects[i])\n",
    "  for pos_idx in position_indexes:\n",
    "    position_data = subject_data.loc[subject_data['position']==pos_idx]\n",
    "\n",
    "    if position_data.shape[0] != 0:\n",
    "\n",
    "      # Remove the 20 first rows for each position if more than 20 samples are present\n",
    "      if position_data.shape[0] > 20:\n",
    "        position_data_cleaned = position_data.iloc[20:,:]\n",
    "      else:\n",
    "        position_data_cleaned = position_data\n",
    "\n",
    "      # Compute and store the averages, min and max\n",
    "      avg_min_max = []\n",
    "      avg_min_max.append(id)\n",
    "      avg_min_max.append(sex)\n",
    "      for column in position_data_cleaned.columns.drop(['position']):\n",
    "        acc_values = position_data_cleaned[column]\n",
    "        avg_min_max.append(acc_values.mean())\n",
    "        avg_min_max.append(min(acc_values))\n",
    "        avg_min_max.append(max(acc_values))\n",
    "\n",
    "      avg_min_max.append(pos_idx)\n",
    "\n",
    "      acceleration_avgs.append(avg_min_max)\n",
    "\n",
    "  id += 1\n",
    "  \n",
    "dataset_avg_min_max = pd.DataFrame(acceleration_avgs, columns = ['subject_id', 'sex', 'x_chest_avg', 'x_chest_min', 'x_chest_max', 'y_chest_avg', 'y_chest_min', 'y_chest_max', 'z_chest_avg', 'z_chest_min', 'z_chest_max', 'x_ankle_avg', 'x_ankle_min', 'x_ankle_max', 'y_ankle_avg', 'y_ankle_min', 'y_ankle_max', 'z_ankle_avg', 'z_ankle_min', 'z_ankle_max', 'position'])\n",
    "\n",
    "dataset_avg_min_max.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZHyeVv3SyrkD",
    "outputId": "0e2a70f2-1353-462e-80dd-c2377bcce8e6"
   },
   "outputs": [],
   "source": [
    "dataset_avg_min_max.subject_id.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nLvtRjHlrxgd"
   },
   "outputs": [],
   "source": [
    "# Define the dataset containing only the averages by selecting the appropriate columns from the previously created dataset\n",
    "\n",
    "dataset_avg = dataset_avg_min_max[['subject_id', 'sex', 'x_chest_avg', 'y_chest_avg', 'z_chest_avg', 'x_ankle_avg', 'y_ankle_avg', 'z_ankle_avg', 'position']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_avg.loc[dataset_avg['subject_id']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 876
    },
    "id": "QjSGiCa3zEZ2",
    "outputId": "97bb3635-0591-43e7-cbe9-529f9fe732bd"
   },
   "outputs": [],
   "source": [
    "# Plot of averaged acceleration data for the 12 consecutive positions for each subject after removing the transient phases\n",
    "\n",
    "fig, axs = plt.subplots(3, 4, figsize=(25,13))\n",
    "axs = axs.flatten()\n",
    "i = 0\n",
    "subject_nb = 1\n",
    "\n",
    "for id in range(1,12):\n",
    "  axs[i].plot(dataset_avg.loc[dataset_avg['subject_id']==id][['position']], dataset_avg.loc[dataset_avg['subject_id']==id][['x_chest_avg', 'y_chest_avg', 'z_chest_avg', 'x_ankle_avg', 'y_ankle_avg', 'z_ankle_avg']])\n",
    "  axs[i].set_title('Subject {}'.format(subject_nb))\n",
    "  axs[i].set_ylabel('Acceleration')\n",
    "  axs[i].set_xlabel('Position')\n",
    "  axs[i].set_xticks([i for i in range(1,13)])\n",
    "  subject_nb += 1\n",
    "  i += 1\n",
    "\n",
    "fig.legend(['x_chest', 'y_chest', 'z_chest', 'x_ankle', 'y_ankle', 'z_ankle'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-AgD1D8FzxFC"
   },
   "outputs": [],
   "source": [
    "dataset.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1lUTosKb04HY",
    "outputId": "a92e14bf-8b8e-43a8-a404-6e006dc29302"
   },
   "outputs": [],
   "source": [
    "# Remove corrupeted data from the 3 datasets - subject_m02 entirely (9)\n",
    "dataset.drop(dataset[dataset.subject_id == 9].index, inplace=True)\n",
    "dataset_avg_min_max.drop(dataset_avg_min_max[dataset_avg_min_max.subject_id == 9].index, inplace=True)\n",
    "dataset_avg.drop(dataset_avg[dataset_avg.subject_id == 9].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "zSJxlQ332xU1",
    "outputId": "20b0d1b7-baf1-4341-d573-2af2cef04f9c"
   },
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d8_IQvBM4guC",
    "outputId": "bc751c7b-0a1b-4d24-fa9e-212ee95ec63e"
   },
   "outputs": [],
   "source": [
    "dataset.shape, dataset_avg.shape, dataset_avg_min_max.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PzTCraBy5kZ8"
   },
   "source": [
    "## Train-test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section aims to split the 3 created datasets in train and test sets, keeping 7 subjects in the train set and 3 subjects in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mFGQT1ya_8jQ",
    "outputId": "2ba538aa-f317-43bf-975a-b16a43285de2"
   },
   "outputs": [],
   "source": [
    "# For dataset\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=1, train_size=.7, random_state=123)\n",
    "gss.get_n_splits()\n",
    "\n",
    "X = dataset.drop(['sex', 'position'], axis=1)\n",
    "position = dataset['position']\n",
    "subject_id = dataset['subject_id']\n",
    "columns = list(dataset.columns)\n",
    "columns.remove('sex')\n",
    "columns.remove('position')\n",
    "\n",
    "# Split with respect to subject_id to prevent a subject from being in both train and test sets\n",
    "for train_idx, test_idx in gss.split(X, position, subject_id):\n",
    "    X_train = np.array(X)[train_idx]\n",
    "    y_train = np.array(position)[train_idx]\n",
    "    X_test = np.array(X)[test_idx]\n",
    "    y_test = np.array(position)[test_idx]\n",
    "\n",
    "X_train = pd.DataFrame(X_train, columns=columns)\n",
    "y_train = pd.DataFrame(y_train, columns=['position'])\n",
    "X_test = pd.DataFrame(X_test, columns=columns)\n",
    "y_test = pd.DataFrame(y_test, columns=['position'])\n",
    "\n",
    "print(X_train['subject_id'].value_counts(), X_test['subject_id'].value_counts())\n",
    "\n",
    "# Concatenate X and y to shuffle the data\n",
    "df_train = pd.concat([X_train, y_train], axis = 1)\n",
    "df_train = df_train.sample(frac=1)\n",
    "df_test = pd.concat([X_test, y_test], axis = 1)\n",
    "df_test = df_test.sample(frac=1)\n",
    "\n",
    "# Shuffle the data\n",
    "X_train = df_train.drop(['position'], axis=1)\n",
    "y_train = df_train[['position']]\n",
    "X_test = df_test.drop(['position'], axis=1)\n",
    "y_test = df_test['position']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ctG8ovfFw3Rv",
    "outputId": "bd18ab1d-5cc6-4999-dbfc-c0638a8f61ff"
   },
   "outputs": [],
   "source": [
    "# For dataset_avg\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=1, train_size=.7, random_state=123)\n",
    "gss.get_n_splits()\n",
    "\n",
    "X = dataset_avg.drop(['sex', 'position'], axis=1)\n",
    "position = dataset_avg['position']\n",
    "subject_id = dataset_avg['subject_id']\n",
    "columns = list(dataset_avg.columns)\n",
    "columns.remove('sex')\n",
    "columns.remove('position')\n",
    "\n",
    "# Split with respect to subject_id to prevent a subject from being in both train and test sets\n",
    "for train_idx, test_idx in gss.split(X, position, subject_id):\n",
    "    X_train_avg = np.array(X)[train_idx]\n",
    "    y_train_avg = np.array(position)[train_idx]\n",
    "    X_test_avg = np.array(X)[test_idx]\n",
    "    y_test_avg = np.array(position)[test_idx]\n",
    "\n",
    "X_train_avg = pd.DataFrame(X_train_avg, columns=columns)\n",
    "y_train_avg = pd.DataFrame(y_train_avg, columns=['position'])\n",
    "X_test_avg = pd.DataFrame(X_test_avg, columns=columns)\n",
    "y_test_avg = pd.DataFrame(y_test_avg, columns=['position'])\n",
    "\n",
    "print(X_train_avg['subject_id'].value_counts(), X_test_avg['subject_id'].value_counts())\n",
    "\n",
    "# Concatenate X and y to shuffle the data\n",
    "df_train_avg = pd.concat([X_train_avg, y_train_avg], axis = 1)\n",
    "df_train_avg = df_train_avg.sample(frac=1)\n",
    "df_test_avg = pd.concat([X_test_avg, y_test_avg], axis = 1)\n",
    "df_test_avg = df_test_avg.sample(frac=1)\n",
    "\n",
    "# Shuffle the data\n",
    "X_train_avg = df_train_avg.drop(['position'], axis=1)\n",
    "y_train_avg = df_train_avg[['position']]\n",
    "X_test_avg = df_test_avg.drop(['position'], axis=1)\n",
    "y_test_avg = df_test_avg['position']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3Ft70jp3hhUi",
    "outputId": "b8d7a107-1a4b-4913-b533-da4cc8b3d403"
   },
   "outputs": [],
   "source": [
    "# For dataset_avg_min_max\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=1, train_size=.7, random_state=123)\n",
    "gss.get_n_splits()\n",
    "\n",
    "X = dataset_avg_min_max.drop(['sex', 'position'], axis=1)\n",
    "position = dataset_avg_min_max['position']\n",
    "subject_id = dataset_avg_min_max['subject_id']\n",
    "columns = list(dataset_avg_min_max.columns)\n",
    "columns.remove('sex')\n",
    "columns.remove('position')\n",
    "\n",
    "# Split with respect to subject_id to prevent a subject from being in both train and test sets\n",
    "for train_idx, test_idx in gss.split(X, position, subject_id):\n",
    "    X_train_avg_min_max = np.array(X)[train_idx]\n",
    "    y_train_avg_min_max = np.array(position)[train_idx]\n",
    "    X_test_avg_min_max = np.array(X)[test_idx]\n",
    "    y_test_avg_min_max = np.array(position)[test_idx]\n",
    "\n",
    "X_train_avg_min_max = pd.DataFrame(X_train_avg_min_max, columns=columns)\n",
    "y_train_avg_min_max = pd.DataFrame(y_train_avg_min_max, columns=['position'])\n",
    "X_test_avg_min_max = pd.DataFrame(X_test_avg_min_max, columns=columns)\n",
    "y_test_avg_min_max = pd.DataFrame(y_test_avg_min_max, columns=['position'])\n",
    "\n",
    "print(X_train_avg_min_max['subject_id'].value_counts(), X_test_avg_min_max['subject_id'].value_counts())\n",
    "\n",
    "# Concatenate X and y to shuffle the data\n",
    "df_train_avg_min_max = pd.concat([X_train_avg_min_max, y_train_avg_min_max], axis = 1)\n",
    "df_train_avg_min_max = df_train_avg_min_max.sample(frac=1)\n",
    "df_test_avg_min_max = pd.concat([X_test_avg_min_max, y_test_avg_min_max], axis = 1)\n",
    "df_test_avg_min_max = df_test_avg_min_max.sample(frac=1)\n",
    "\n",
    "# Shuffle the data\n",
    "X_train_avg_min_max = df_train_avg_min_max.drop(['position'], axis=1)\n",
    "y_train_avg_min_max = df_train_avg_min_max[['position']]\n",
    "X_test_avg_min_max = df_test_avg_min_max.drop(['position'], axis=1)\n",
    "y_test_avg_min_max = df_test_avg_min_max['position']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qagYeTqmI4rK"
   },
   "source": [
    "## Dataset standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training the models, we need to standardize the three created datasets. \\\n",
    "It is important to learn the scaler on the train set only and to use it to scale both the train and test sets, to avoid including information from the test set in the training phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IHy9KEyVI66c"
   },
   "outputs": [],
   "source": [
    "# For dataset\n",
    "# Learn scaler on train set\n",
    "scaler = StandardScaler().fit(X_train.iloc[:,:-1])\n",
    "\n",
    "# Standardize train set\n",
    "X_train.iloc[:,:-1] = scaler.transform(X_train.iloc[:,:-1])\n",
    "\n",
    "# Standardize test set\n",
    "X_test.iloc[:,:-1] = scaler.transform(X_test.iloc[:,:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "1YVZRYgnJUXy",
    "outputId": "23e3575a-bd2c-43a3-ba65-79d9b00674e1"
   },
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zQfvcWz-0rpZ"
   },
   "outputs": [],
   "source": [
    "# For dataset_avg\n",
    "# Learn scaler on train set\n",
    "scaler = StandardScaler().fit(X_train_avg.iloc[:,1:])\n",
    "\n",
    "# Standardize train set\n",
    "X_train_avg.iloc[:,1:] = scaler.transform(X_train_avg.iloc[:,1:])\n",
    "\n",
    "# Standardize test set\n",
    "X_test_avg.iloc[:,1:] = scaler.transform(X_test_avg.iloc[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Z3Se3Vt0rvW"
   },
   "outputs": [],
   "source": [
    "# For dataset_avg_min_max\n",
    "# Learn scaler on train set\n",
    "scaler = StandardScaler().fit(X_train_avg_min_max.iloc[:,1:])\n",
    "\n",
    "# Standardize train set\n",
    "X_train_avg_min_max.iloc[:,1:] = scaler.transform(X_train_avg_min_max.iloc[:,1:])\n",
    "\n",
    "# Standardize test set\n",
    "X_test_avg_min_max.iloc[:,1:] = scaler.transform(X_test_avg_min_max.iloc[:,1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oA31pZTbHlQp"
   },
   "source": [
    "## Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section presents the hyperp_search function used to perform grid search and cross-validation, followed by all the models used for training, the best model found (parameters combination) for each of them and the performances on the test set. \\\n",
    "At first, all the models are performed on the third dataset, and then the best ones are also applied to datasets 1 and 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "650jylmRb8dB"
   },
   "outputs": [],
   "source": [
    "def hyperp_search(classifier, parameters, k, X_train, y_train, X_test, y_test):\n",
    "    ''' This function takes as input a classifier (e.g. Decision Tree), a dictionary of parameters, the number of folds k for the cross-valdiation,\n",
    "                                     the train (X_train, y_train) and the test (X_test, y_test) sets\n",
    "                      performs a grid search using the cross-validation method and stores the best set of parameters looking at the performances on the validation sets\n",
    "                      prints the different metrics on the train and the test sets\n",
    "                      returns the best model fitted (refit=True) on the best set of parameters found\n",
    "    '''\n",
    "\n",
    "    # Hyperparameter tuning via cross-validation\n",
    "    gs = GridSearchCV(classifier, parameters, cv=k, scoring = 'accuracy', refit=True, verbose=0, n_jobs=-1) # refit=True refits the model with the best found parameters on the whole dataset\n",
    "    gs = gs.fit(X_train, y_train)\n",
    "\n",
    "    # Prediction on train and test sets with best model found\n",
    "    best_model = gs.best_estimator_\n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "    y_train_pred = best_model.predict(X_train)\n",
    "\n",
    "    # Results display\n",
    "    print(\"\\n \\033[1m ***Best result obtained*** \\033[0m \\n\")\n",
    "    print(\"accuracy (mean cross-validated score): %f using %s\" % (gs.best_score_, gs.best_params_))\n",
    "\n",
    "    print(\"\\n \\033[1m ***Scores obtained on train and test sets with best model*** \\033[0m \\n\")\n",
    "    print(\"f1          train %.3f   test %.3f\" % (f1_score(y_train, y_train_pred, average='macro'), f1_score(y_test, y_test_pred, average='macro'))) \n",
    "    print(\"recall      train %.3f   test %.3f\" % (recall_score(y_train, y_train_pred, average='macro'), recall_score(y_test, y_test_pred, average='macro'))) \n",
    "    print(\"precision   train %.3f   test %.3f\" % (precision_score(y_train, y_train_pred, average='macro'), precision_score(y_test, y_test_pred, average='macro'))) \n",
    "    print(\"accuracy    train %.3f   test %.3f\" % (accuracy_score(y_train, y_train_pred), accuracy_score(y_test, y_test_pred))) \n",
    "\n",
    "    print(\"\\n \\033[1m ***Classification report on test set*** \\033[0m \\n\")\n",
    "    print(classification_report(y_test, y_test_pred, target_names=['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12']))\n",
    "\n",
    "    print(\"\\n \\033[1m ***Confusion matrix on test set*** \\033[0m \\n\")\n",
    "    plot_confusion_matrix(best_model, X_test, y_test, display_labels=['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12'], values_format='', cmap='Reds')\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FMaw7EUT3Krz"
   },
   "source": [
    "### On dataset with averages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SPyWo4qKG_TB"
   },
   "source": [
    "#### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qL78XRDLG76K",
    "outputId": "65633565-b605-40c1-8bf5-e625b2489f07"
   },
   "outputs": [],
   "source": [
    "classifier = KNeighborsClassifier()\n",
    "parameters = {'n_neighbors':np.arange(3,50,1)}\n",
    "\n",
    "best_model_knn_avg = hyperp_search(classifier, parameters, 5, X_train_avg, y_train_avg, X_test_avg, y_test_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zgd_UVUHMxPi"
   },
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "cjOCqVPjMzbd",
    "outputId": "24bcc4ed-7c4b-48a6-8279-52db3dbafdfa"
   },
   "outputs": [],
   "source": [
    "classifier = DecisionTreeClassifier()\n",
    "parameters = {'criterion': ['entropy','gini'], \n",
    "              'max_depth': [4,5,6],\n",
    "              'min_samples_split': [3,5],\n",
    "              'min_samples_leaf': [3,5]}\n",
    "\n",
    "best_model_tree_avg = hyperp_search(classifier, parameters, 5, X_train_avg, y_train_avg, X_test_avg, y_test_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "REzldoHzxG3H",
    "outputId": "a946dfa1-8956-4d0e-8c6b-699638ced84a"
   },
   "outputs": [],
   "source": [
    "r = tree.export_text(best_model_tree_avg, feature_names=X_test_avg.columns.tolist(), max_depth=5)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4pVFnu5ID9C"
   },
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "STBQOEi-IG6i",
    "outputId": "c2b8b4e0-80a1-4d4b-8d6c-b623b676f100"
   },
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier()\n",
    "parameters = {'n_estimators': [10, 20], \n",
    "              'criterion': ['entropy', 'gini'], \n",
    "              'max_depth': [4, 5, 6],\n",
    "              'min_samples_split': [3, 5],   \n",
    "              'min_samples_leaf': [3, 5]}  \n",
    "\n",
    "best_model_rf_avg = hyperp_search(classifier, parameters, 7, X_train_avg, y_train_avg, X_test_avg, y_test_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RrbAeNsRIQ5C"
   },
   "source": [
    "#### Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "E-LgjuwQITaL",
    "outputId": "05aeb0e0-c03a-4108-ec5b-082f2c75a021"
   },
   "outputs": [],
   "source": [
    "classifier= AdaBoostClassifier()\n",
    "parameters = {'n_estimators' : np.arange(1500,2500,500),\n",
    "              'learning_rate' : [0.001,0.01,0.1]}\n",
    "\n",
    "best_model_adaboost_avg = hyperp_search(classifier, parameters, 5, X_train_avg, y_train_avg, X_test_avg, y_test_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UjBzpSUxC0g8"
   },
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "SzFZleJrC5U1",
    "outputId": "cb4a5eae-ad81-4b8c-8a20-b7fb1424b037"
   },
   "outputs": [],
   "source": [
    "classifier = xgb.XGBClassifier()\n",
    "parameters = {'max_depth': [3,4,5], \n",
    "              'n_estimators': [10,20,30], \n",
    "              'reg_lambda': [0,1,2],\n",
    "              'reg_alpha': [0,1,2],\n",
    "              'gamma': [1,2,3]}\n",
    "\n",
    "best_model_xgb_avg = hyperp_search(classifier, parameters, 5, X_train_avg, y_train_avg, X_test_avg, y_test_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I0q93qmkIYhi"
   },
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "BV23f-cPIcAC",
    "outputId": "81f4f92e-2bb2-464e-fd03-983ee1432b03"
   },
   "outputs": [],
   "source": [
    "classifier = LogisticRegression()\n",
    "parameters = {\"C\":[0.01,0.1,1],\n",
    "              \"max_iter\":[10000],\n",
    "              \"penalty\":['l2']}\n",
    "\n",
    "best_model_lr_avg = hyperp_search(classifier, parameters, 5, X_train_avg, y_train_avg, X_test_avg, y_test_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ox7lCCQIkQA"
   },
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "o_lFkAuMInWt",
    "outputId": "a6c567b4-0812-4d8b-c7d4-92013c50a1ce"
   },
   "outputs": [],
   "source": [
    "classifier = SVC()\n",
    "parameters = {\"kernel\":['linear'],\n",
    "              \"C\":[0.01,0.1]}\n",
    "\n",
    "best_model_svm_avg = hyperp_search(classifier, parameters, 5, X_train_avg, y_train_avg, X_test_avg, y_test_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Ag97y_yIpHU"
   },
   "source": [
    "#### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ackLcmlDIvFG",
    "outputId": "144dddef-7245-454e-ca36-ec3eca85d36b"
   },
   "outputs": [],
   "source": [
    "classifier = MLPClassifier()\n",
    "parameters = {\"hidden_layer_sizes\":[(16,8), (10,8,5)],  \"max_iter\": [500, 1000], \"alpha\": [1]}\n",
    "\n",
    "best_model_mlp_avg = hyperp_search(classifier, parameters, 5, X_train_avg, y_train_avg, X_test_avg, y_test_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZPIijE8P3b_p"
   },
   "source": [
    "### On dataset with raw samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4V0gn_x4mls"
   },
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "7uBj-j114oV-",
    "outputId": "91e100eb-aa4d-4518-cbc7-9dcf75e45640"
   },
   "outputs": [],
   "source": [
    "classifier = DecisionTreeClassifier()\n",
    "parameters = {'criterion': ['entropy','gini'], \n",
    "              'max_depth': [4,5,6],\n",
    "              'min_samples_split': [3,5],\n",
    "              'min_samples_leaf': [3,5]}\n",
    "\n",
    "best_model_tree_raw = hyperp_search(classifier, parameters, 5, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zrlq7SvO4sM-"
   },
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Jt0PB-7P4wIB",
    "outputId": "dc2d9eb4-9694-412d-fdfd-ef0cde07bef4"
   },
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier()\n",
    "parameters = {'n_estimators': [10, 20], \n",
    "              'criterion': ['entropy', 'gini'], \n",
    "              'max_depth': [4, 5, 6],\n",
    "              'min_samples_split': [3, 5],   \n",
    "              'min_samples_leaf': [3, 5]}  \n",
    "\n",
    "best_model_rf_raw = hyperp_search(classifier, parameters, 7, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4b90sRB7Zw0"
   },
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "6vwh5s807c6x",
    "outputId": "5f2c2568-523e-4958-cc8c-de2782d502b1"
   },
   "outputs": [],
   "source": [
    "classifier = SVC()\n",
    "parameters = {\"kernel\":['linear'],\n",
    "              \"C\":[0.01,0.1]}\n",
    "\n",
    "best_model_svm_raw = hyperp_search(classifier, parameters, 5, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TzZ5Kvpy5BDY"
   },
   "source": [
    "Keeping all the data is increasing overfitting without meliorating the results. \\\n",
    "Indeed, this result was predictable since by keeping all the data recorded for each position, we are feeding the network the same data many times, making it more prone to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5sPdcEVm3hdY"
   },
   "source": [
    "### On dataset with averages, min and max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_C3Q8NGp5IdE"
   },
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "25yr3cgv5K6H",
    "outputId": "dcb0be4c-6253-4382-88e2-cbb728d61462"
   },
   "outputs": [],
   "source": [
    "classifier = DecisionTreeClassifier()\n",
    "parameters = {'criterion': ['entropy','gini'], \n",
    "              'max_depth': [4,5,6],\n",
    "              'min_samples_split': [3,5],\n",
    "              'min_samples_leaf': [3,5]}\n",
    "\n",
    "best_model_tree_avg_min_max = hyperp_search(classifier, parameters, 5, X_train_avg_min_max, y_train_avg_min_max, X_test_avg_min_max, y_test_avg_min_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JfPXMtr45SZh"
   },
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "M9I7GLL45UkD",
    "outputId": "dac4aeeb-4055-4525-a9b7-6e66d8bae500"
   },
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier()\n",
    "parameters = {'n_estimators': [10, 20], \n",
    "              'criterion': ['entropy', 'gini'], \n",
    "              'max_depth': [4, 5, 6],\n",
    "              'min_samples_split': [3, 5],   \n",
    "              'min_samples_leaf': [3, 5]}  \n",
    "\n",
    "best_model_rf_avg_min_max = hyperp_search(classifier, parameters, 7, X_train_avg_min_max, y_train_avg_min_max, X_test_avg_min_max, y_test_avg_min_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xTKm2zVP7lVO"
   },
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "jrw-dlpU7raz",
    "outputId": "0bb02fa8-e39a-447a-8bd7-13d43d1a596d"
   },
   "outputs": [],
   "source": [
    "classifier = SVC()\n",
    "parameters = {\"kernel\":['linear'],\n",
    "              \"C\":[0.01,0.1]}\n",
    "\n",
    "best_model_svm_avg_min_max = hyperp_search(classifier, parameters, 5, X_train_avg_min_max, y_train_avg_min_max, X_test_avg_min_max, y_test_avg_min_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best result obtained is with Random Forest applied to this dataset, with which we are reaching 50% of accuracy on the test set. Even if the gap between train and test metrics is significant, there is no strong overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ueSud95wSjF4"
   },
   "source": [
    "## Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we take a look at unsupervised learning techniques. Even though the nature of our problem doesn't require us to use unsupervised approaches, since we have labels for our data points, it still is of interest to evaluate different models and see their performance. \n",
    "\n",
    "We take a closer look at the K-means algorithm, hierarchical as well as density-based clustering and the Gaussian Mixture Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-HkZxu8wa1do"
   },
   "outputs": [],
   "source": [
    "X = pd.concat([X_train, X_test], axis=0, ignore_index=True)\n",
    "y_test = pd.DataFrame(y_test, columns=['position'])\n",
    "y = pd.concat([y_train, y_test], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gshRFVOYTCJJ"
   },
   "source": [
    "### K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MS-aOvVVd9Qi",
    "outputId": "ac275969-cf88-4cde-d8ed-e8227593a44c"
   },
   "outputs": [],
   "source": [
    "classifier = KMeans(12)\n",
    "classifier.fit(X)\n",
    "\n",
    "y_pred = classifier.predict(X)\n",
    "\n",
    "# Calculate some metrics\n",
    "print('The adjusted Rand index has a value of {0:.4f}'.format(adjusted_rand_score(np.array(y.position), y_pred)))\n",
    "print('The Normalized Mutual Information has a value of {0:.4f}'.format(normalized_mutual_info_score(np.array(y.position), y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "iAKfICI4T0_-",
    "outputId": "59737345-0b1a-403d-f522-033d4289f20c"
   },
   "outputs": [],
   "source": [
    "wcss = []\n",
    "#this loop will fit the k-means algorithm to our data and \n",
    "#second we will compute the within cluster sum of squares and #appended to our wcss list\n",
    "\n",
    "for i in range(1,13): \n",
    "  kmeans = KMeans(n_clusters=i, init ='k-means++', max_iter=300, n_init=10, random_state=0 )\n",
    "  kmeans.fit(X_train)\n",
    "  wcss.append(kmeans.inertia_)  #kmeans inertia_ attribute is:  Sum of squared distances of samples #to their closest cluster center\n",
    "\n",
    "# Plot the elbow graph\n",
    "plt.plot(range(1,13), wcss)\n",
    "plt.title('The Elbow Method Graph')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Clustering: Ward's Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "id": "kGVydmU_XPI-",
    "outputId": "14435f38-1325-44f5-f38f-286ab4dc27d6"
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "model = AgglomerativeClustering(linkage='ward', distance_threshold=12, n_clusters=None)\n",
    "y_pred = model.fit_predict(X)\n",
    "\n",
    "def plot_dendrogram(model, **kwargs):\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "\n",
    "    # create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack(\n",
    "        [model.children_, model.distances_, counts]\n",
    "    ).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, **kwargs)\n",
    "\n",
    "plt.title(\"Hierarchical Clustering Dendrogram\")\n",
    "# Plot the top three levels of the dendrogram\n",
    "plot_dendrogram(model, truncate_mode=\"level\", p=3)\n",
    "plt.xlabel(\"Number of points in node (or index of point if no parenthesis).\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate some metrics\n",
    "print('The adjusted Rand index has a value of {0:.4f}'.format(adjusted_rand_score(np.array(y.position), y_pred)))\n",
    "print('The Normalized Mutual Information has a value of {0:.4f}'.format(normalized_mutual_info_score(np.array(y.position), y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Density-based Clustering: DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = DBSCAN(eps=0.5, min_samples=5)\n",
    "\n",
    "# Fit model and make predictions\n",
    "y_pred = model.fit_predict(X)\n",
    "\n",
    "# Calculate some metrics\n",
    "print('The adjusted Rand index has a value of {0:.4f}'.format(adjusted_rand_score(np.array(y.position), y_pred)))\n",
    "print('The Normalized Mutual Information has a value of {0:.4f}'.format(normalized_mutual_info_score(np.array(y.position), y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Mixture Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wBaMerHE85o6"
   },
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = GaussianMixture(n_components=12)\n",
    "\n",
    "# Fit the model and make predictions\n",
    "y_pred = model.fit_predict(X)\n",
    "\n",
    "# Calculate some metrics\n",
    "print('The adjusted Rand index has a value of {0:.4f}'.format(adjusted_rand_score(np.array(y.position), y_pred)))\n",
    "print('The Normalized Mutual Information has a value of {0:.4f}'.format(normalized_mutual_info_score(np.array(y.position), y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions for unsupervised learning\n",
    "\n",
    "Since we have labels we can compare our predictions to, we used the adjusted Rand index and the Normalized Mutual Information in order to evaluate our models. The best ones are the K-means algorithm as well as the GMM. \n",
    "\n",
    "Both perform very similar to each other regarding the respective index. This comes also because of the face that we have given the number of clusters to find as input (n = 12)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ORintr4ByPI"
   },
   "source": [
    "## Positions grouping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2XuPmuoD88h6"
   },
   "source": [
    "In this section we are grouping the positions in 4 master categories, corresponding to supine, prone, left and right positions. \\\n",
    "We are aiming to train a classifier able to classify perfectly the data in 4 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OC8GolsrB5vy"
   },
   "outputs": [],
   "source": [
    "grouped_dataset = dataset_avg_min_max.copy()\n",
    "\n",
    "grouped_dataset.loc[grouped_dataset.position == 2, 'position'] = 1\n",
    "grouped_dataset.loc[grouped_dataset.position == 3, 'position'] = 1\n",
    "grouped_dataset.loc[grouped_dataset.position == 4, 'position'] = 2\n",
    "grouped_dataset.loc[grouped_dataset.position == 5, 'position'] = 2\n",
    "grouped_dataset.loc[grouped_dataset.position == 6, 'position'] = 2\n",
    "grouped_dataset.loc[grouped_dataset.position == 7, 'position'] = 3\n",
    "grouped_dataset.loc[grouped_dataset.position == 8, 'position'] = 3\n",
    "grouped_dataset.loc[grouped_dataset.position == 9, 'position'] = 3\n",
    "grouped_dataset.loc[grouped_dataset.position == 10, 'position'] = 4\n",
    "grouped_dataset.loc[grouped_dataset.position == 11, 'position'] = 4\n",
    "grouped_dataset.loc[grouped_dataset.position == 12, 'position'] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "culLNfzGDZKU",
    "outputId": "1402d826-d7e6-481b-e0da-931814a8f79b"
   },
   "outputs": [],
   "source": [
    "grouped_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sWs1R5fHCn6S",
    "outputId": "075d932a-10ea-4943-b881-a2cce6937a9a"
   },
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=1, train_size=.7, random_state=123)\n",
    "gss.get_n_splits()\n",
    "\n",
    "X = grouped_dataset.drop(['sex', 'position'], axis=1)\n",
    "position = grouped_dataset['position']\n",
    "subject_id = grouped_dataset['subject_id']\n",
    "columns = list(grouped_dataset.columns)\n",
    "columns.remove('sex')\n",
    "columns.remove('position')\n",
    "\n",
    "# Split with respect to subject_id to prevent a subject from being in both train and test sets\n",
    "for train_idx, test_idx in gss.split(X, position, subject_id):\n",
    "    X_train_grp = np.array(X)[train_idx]\n",
    "    y_train_grp = np.array(position)[train_idx]\n",
    "    X_test_grp = np.array(X)[test_idx]\n",
    "    y_test_grp = np.array(position)[test_idx]\n",
    "\n",
    "X_train_grp = pd.DataFrame(X_train_grp, columns=columns)\n",
    "y_train_grp = pd.DataFrame(y_train_grp, columns=['position'])\n",
    "X_test_grp = pd.DataFrame(X_test_grp, columns=columns)\n",
    "y_test_grp = pd.DataFrame(y_test_grp, columns=['position'])\n",
    "\n",
    "print(X_train_grp['subject_id'].value_counts(), X_test_grp['subject_id'].value_counts())\n",
    "\n",
    "# Concatenate X and y to shuffle the data\n",
    "df_train_grp = pd.concat([X_train_grp, y_train_grp], axis = 1)\n",
    "df_train_grp = df_train_grp.sample(frac=1)\n",
    "df_test_grp = pd.concat([X_test_grp, y_test_grp], axis = 1)\n",
    "df_test_grp = df_test_grp.sample(frac=1)\n",
    "\n",
    "# Shuffle the data\n",
    "X_train_grp = df_train_grp.drop(['position'], axis=1)\n",
    "y_train_grp = df_train_grp[['position']]\n",
    "X_test_grp = df_test_grp.drop(['position'], axis=1)\n",
    "y_test_grp = df_test_grp['position']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YzfTOTPMKapd"
   },
   "outputs": [],
   "source": [
    "# Learn scaler on train set\n",
    "scaler = StandardScaler().fit(X_train_grp.iloc[:,1:])\n",
    "\n",
    "# Standardize train set\n",
    "X_train_grp.iloc[:,1:] = scaler.transform(X_train_grp.iloc[:,1:])\n",
    "\n",
    "# Standardize test set\n",
    "X_test_grp.iloc[:,1:] = scaler.transform(X_test_grp.iloc[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "btqNZimuD9w1"
   },
   "outputs": [],
   "source": [
    "def hyperp_search_grp(classifier, parameters, k, X_train, y_train, X_test, y_test):\n",
    "    ''' This function takes as input a classifier (e.g. Decision Tree), a dictionary of parameters, the number of folds k for the cross-valdiation,\n",
    "                                     the train (X_train, y_train) and the test (X_test, y_test) sets\n",
    "                      performs a grid search using the cross-validation method and stores the best set of parameters looking at the performances on the validation sets\n",
    "                      prints the different metrics on the train and the test sets\n",
    "                      returns the best model fitted (refit=True) on the best set of parameters found\n",
    "    '''\n",
    "\n",
    "    # Hyperparameter tuning via cross-validation\n",
    "    gs = GridSearchCV(classifier, parameters, cv=k, scoring = 'accuracy', refit=True, verbose=0, n_jobs=-1) # refit=True refits the model with the best found parameters on the whole dataset\n",
    "    gs = gs.fit(X_train, y_train)\n",
    "\n",
    "    # Prediction on train and test sets with best model found\n",
    "    best_model = gs.best_estimator_\n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "    y_train_pred = best_model.predict(X_train)\n",
    "\n",
    "    # Results display\n",
    "    print(\"\\n \\033[1m ***Best result obtained*** \\033[0m \\n\")\n",
    "    print(\"accuracy (mean cross-validated score): %f using %s\" % (gs.best_score_, gs.best_params_))\n",
    "\n",
    "    print(\"\\n \\033[1m ***Scores obtained on train and test sets with best model*** \\033[0m \\n\")\n",
    "    print(\"f1          train %.3f   test %.3f\" % (f1_score(y_train, y_train_pred, average='macro'), f1_score(y_test, y_test_pred, average='macro'))) \n",
    "    print(\"recall      train %.3f   test %.3f\" % (recall_score(y_train, y_train_pred, average='macro'), recall_score(y_test, y_test_pred, average='macro'))) \n",
    "    print(\"precision   train %.3f   test %.3f\" % (precision_score(y_train, y_train_pred, average='macro'), precision_score(y_test, y_test_pred, average='macro'))) \n",
    "    print(\"accuracy    train %.3f   test %.3f\" % (accuracy_score(y_train, y_train_pred), accuracy_score(y_test, y_test_pred))) \n",
    "\n",
    "    print(\"\\n \\033[1m ***Classification report on test set*** \\033[0m \\n\")\n",
    "    print(classification_report(y_test, y_test_pred, target_names=['1', '2', '3', '4']))\n",
    "\n",
    "    print(\"\\n \\033[1m ***Confusion matrix on test set*** \\033[0m \\n\")\n",
    "    plot_confusion_matrix(best_model, X_test, y_test, display_labels=['1', '2', '3', '4'], values_format='', cmap='Reds')\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 859
    },
    "id": "W21JD5a-Dw8A",
    "outputId": "3f9d933d-a1e7-42d7-d4ef-a43906d3f0e4"
   },
   "outputs": [],
   "source": [
    "classifier = KNeighborsClassifier()\n",
    "parameters = {'n_neighbors':np.arange(3,50,1)}\n",
    "\n",
    "best_model_knn_grp = hyperp_search_grp(classifier, parameters, 5, X_train_grp, y_train_grp, X_test_grp, y_test_grp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 824
    },
    "id": "-2544gidExjc",
    "outputId": "38fea30a-b444-45ad-af1a-80f1335f231d"
   },
   "outputs": [],
   "source": [
    "classifier = DecisionTreeClassifier()\n",
    "parameters = {'criterion': ['entropy','gini'], \n",
    "              'max_depth': [4, 5, 6],\n",
    "              'min_samples_split': [3, 5],\n",
    "              'min_samples_leaf': [3, 5]}\n",
    "\n",
    "best_model_tree_grp = hyperp_search_grp(classifier, parameters, 5, X_train_grp, y_train_grp, X_test_grp, y_test_grp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 859
    },
    "id": "fFSDX9e9G3HY",
    "outputId": "02688b30-b747-4c2a-e073-4279001b75d0"
   },
   "outputs": [],
   "source": [
    "classifier = SVC()\n",
    "parameters = {\"kernel\":['linear'],\n",
    "              \"C\":[0.01,0.1]}\n",
    "\n",
    "best_model_svm_grp = hyperp_search_grp(classifier, parameters, 5, X_train_grp, y_train_grp, X_test_grp, y_test_grp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the best found models, we are able to perfectly predict the 4 master positions of the test set."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "1a4KEXnKmApS",
    "gtiRkxxGmI8W",
    "47_bFUv1mMBN",
    "qCoBK2tzpXLD",
    "PzTCraBy5kZ8",
    "qagYeTqmI4rK",
    "oA31pZTbHlQp",
    "SPyWo4qKG_TB",
    "Zgd_UVUHMxPi",
    "a4pVFnu5ID9C",
    "RrbAeNsRIQ5C",
    "UjBzpSUxC0g8",
    "I0q93qmkIYhi",
    "3ox7lCCQIkQA",
    "4Ag97y_yIpHU",
    "a4V0gn_x4mls",
    "Zrlq7SvO4sM-",
    "a4b90sRB7Zw0",
    "_C3Q8NGp5IdE",
    "JfPXMtr45SZh",
    "xTKm2zVP7lVO",
    "ueSud95wSjF4",
    "4ORintr4ByPI",
    "j5CzRbGwIxuI"
   ],
   "name": "Sleep_classifier_v1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:pcb]",
   "language": "python",
   "name": "conda-env-pcb-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "dcaa35b7e8eb6dc758a017cfbc98c8f33fb7601f0e069f72bb1b5e0908b2c370"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
